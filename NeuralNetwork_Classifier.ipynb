{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Neural Network Classifier Python Implementation #\n",
    "\n",
    "Here I present a python implementation of a Neural Network from scratch. I started from the lectures and (MATLAB) exercises on Neural Networks from Andrew NG course \"Machine Learning\" available on Coursera.\n",
    "I then expand the options. In this implementation you can add new layer and units on the Neural Network without the need to modify the code. This is a mere exercise and it is not intended as state of the art. Still the results are pretty satisfying and I have anjoyed a lot doing it. I have tested the algorithm on hand writing recognition data provided by the already mentioned \"Machine Learning\" Course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import fmin_cg\n",
    "\n",
    "def sigmoid(X):\n",
    "    \"\"\"\n",
    "    It returns the sigmoid function applied to the matrix X. X may have any shape.\n",
    "    \"\"\"\n",
    "    sig = 1/(1+np.exp(-X))\n",
    "    return sig\n",
    "\n",
    "def sigmoidGradient(X):\n",
    "    \"\"\"\n",
    "    It returns the derivative of the sigmoid function calculated at X. X is a matrix and may have any shape.\n",
    "    \"\"\"\n",
    "    sig_g = sigmoid(X)*(1-sigmoid(X))\n",
    "    return sig_g\n",
    "\n",
    "def Rand_param_Theta(In_value, Out_value):\n",
    "    \"\"\"\n",
    "    It returns a (Out_value x In_value+1) random matrix with small entries.\n",
    "    \"\"\"\n",
    "    epsilon_init = 0.12\n",
    "    Rand_param = np.random.rand(Out_value, In_value+1) * 2 * epsilon_init - epsilon_init\n",
    "    return Rand_param\n",
    "\n",
    "\n",
    "def nn_predict(nn_params, layers_data, X, min_class_pred = 0):\n",
    "    \"\"\"\n",
    "    The function returns a vector with the classification prediction for the set X.\n",
    "    min_class_pred identify the smaller integer in the classification and must be adjusted accordingly to your\n",
    "    classification labels. For instance if your labels are 1-10 you need to set min_class_pred =1. This is because\n",
    "    the predictions are obtained from the indeces of a matrix and python as base index 0. By default it is setted\n",
    "    on zero.\n",
    "    \"\"\"\n",
    "    Theta_vec = reshape_thetas(nn_params, layers_data) \n",
    "    m = np.shape(X)[0]\n",
    "    h_pred = [0 for i in range(len(Theta_vec)+1)]\n",
    "    h_pred[0] = np.hstack((np.ones(m)[np.newaxis].T, X))\n",
    "    for i in range(len(h_pred)-1):\n",
    "        h = sigmoid(h_pred[i]@Theta_vec[i].T)\n",
    "        if i == len(h_pred)-2:\n",
    "\n",
    "            h_pred[i+1] = h                                           # h_pred[-1] is (num_exp x num_label) matrix. The entry (i,j) contains the probabilities\n",
    "                                                                      # that the i-th experiment is classified as label j\n",
    "        else:\n",
    "            h_pred[i+1] = np.hstack((np.ones(m)[np.newaxis].T, h))\n",
    "\n",
    "    return np.argmax(h_pred[-1],axis = 1)+min_class_pred\n",
    "\n",
    "def reshape_thetas(nn_params,layers_data):\n",
    "    \"\"\"\n",
    "    Given a vector v and a list of integers L of lenght n it will returns a list of n-1 matrices exatrapolated by the vector v.\n",
    "    The shape of a returned matrix is (j x (i+1)) where [...,i,j,..] are two consecutive entries of the list L.\n",
    "    If it will not be possible automatically rise an Error. The order to reshape the matrices is F = Fortran.\n",
    "    \"\"\"\n",
    "    T_list = []\n",
    "    start_ind = 0\n",
    "    for i in range(len(layers_data)-1):\n",
    "        n = layers_data[i+1]\n",
    "        m = layers_data[i] + 1\n",
    "        dum = np.reshape(nn_params[start_ind:start_ind+n*m], (n, m), order = 'F')\n",
    "        T_list.append(dum)\n",
    "        start_ind = start_ind + n*m\n",
    "    return T_list\n",
    "\n",
    "def  ForwardProp(nn_params, layers_data, X, Y):\n",
    "    \"\"\"\n",
    "    It compute the Forward Propagation of the Neural Network. It returns \n",
    "            -a_vec : a vector containing the computed value of the units for each layer.\n",
    "                    Therefore a_vec[0] are the input units + bias, while a_vec[-1] are\n",
    "                    the outputs units without the bias to use in the activation function.\n",
    "\n",
    "           - z_vec : contains the values of the activation function at each unit of the layer\n",
    "                     excluded the output ones.\n",
    "\n",
    "           - Y : is a label Matrix of zeroes and ones. Column j has value 1\n",
    "                 in i-th position if i is the value of the j-th entries of Y.\n",
    "\n",
    "           - Theta_vec : is the list of the weights matrices.\n",
    "    \"\"\"\n",
    "                ### UTILITIES ###\n",
    "    Theta_vec = reshape_thetas(nn_params, layers_data)                 # recover the weights matrices from the weights vector\n",
    "\n",
    "    m = np.shape(X)[0]                                                 # number of experiments\n",
    "    min_class = np.unique(Y)[0]                                        # smallest value occurring in the classification\n",
    "    max_class = np.unique(Y)[-1]                                       # larger alue occurring in the classification\n",
    "    I = np.repeat([np.arange(min_class, max_class +1)],m,0).T          # Classification Matrix\n",
    "    Y = Y.reshape(Y.size)                                              # Reshape Y in a row vector in order to perform boolean operation\n",
    "    Y = np.array(I == Y).astype(int)                                   # Matrix of zeroes and ones of the labels. Column j has value 1 in i-th position if i is the value of the j-th entries of Y. \n",
    "\n",
    "\n",
    "    ### FORWARD ROPAGATION ###\n",
    "    J = 0                                                  # Initialise the cost Function J\n",
    "    a1 = np.hstack((np.ones(m)[np.newaxis].T, X))          # Add the bias Column at the Input Layer\n",
    "    a_vec = [0 for i in range(len(layers_data))]           # Initialise the units vector\n",
    "    z_vec = [0 for i in range(len(layers_data)-1)]         # Initialise the activation function vector\n",
    "    a_vec[0] = a1.T\n",
    "\n",
    "    for i in range(len(layers_data)-1):\n",
    "        zi = Theta_vec[i]@a_vec[i]                  # comupte z(i)\n",
    "        z_vec[i] = zi\n",
    "        ai = sigmoid(zi)                            # Values of the unit at layer (i)\n",
    "        mi = np.shape(ai)[1]\n",
    "        if i != len(layers_data)-2:\n",
    "            a_i_bias = np.vstack((np.ones(mi), ai)) # Add the Bias Column at the units\n",
    "            a_vec[i+1] = a_i_bias\n",
    "        else:\n",
    "            a_vec[i+1] = ai\n",
    "\n",
    "    return a_vec, z_vec, Y,Theta_vec\n",
    "\n",
    "\n",
    "def cost_func_NN(nn_params, layers_data, X, Y, Lambda =0):\n",
    "    \"\"\"\n",
    "      The function has input:\n",
    "        - nn params  : a vector of weights parameters\n",
    "        - layers_data: a vector containing the number of units at each layer (input and output included)\n",
    "        - X : the training set matrix X\n",
    "        - Y : the label vector Y\n",
    "        - J : the regularisation parameter, by default is zero\n",
    "     The algorithm is vectorised and will return the value of the cost function J on the output layer.\n",
    "    \"\"\"\n",
    "\n",
    "    m = np.shape(X)[0]\n",
    "    a,_,Y,Theta_vec = ForwardProp(nn_params, layers_data, X, Y)   #computing the units values, the label matrix Y and the weights matrices\n",
    "    a_out = a[-1]  # the output units of the Neural Network\n",
    "\n",
    "    reg = (Lambda/(2*m))*(sum([(Theta[:,1:]*Theta[:,1:]).sum() for Theta in Theta_vec])) #regolarisation factor\n",
    "\n",
    "    J = (1/m) * (((-Y * np.log(a_out))-((1-Y) * np.log(1-a_out))).sum()) + reg           #formula for the regularised cost function\n",
    "\n",
    "    return(J)\n",
    "\n",
    "\n",
    "\n",
    "def grad_NN(nn_params,layers_data, X, Y, Lambda =0):\n",
    "    \"\"\"\n",
    "    It compute the Backward Propagation and gradient of the Neural Network.\n",
    "    The function as input\n",
    "        - nn params  : a vector of weights parameters\n",
    "        - layers_data: a vector containing the number of units at each layer (input and output included)\n",
    "        - X : the training set matrix X\n",
    "        - Y : the label vector Y\n",
    "        - J : the regularisation parameter, by default is zero\n",
    "   The output is a single vector containing all the unrolled gradients. This is because the fmin_cg accept\n",
    "   only vectors and not matrices.\n",
    "   \"\"\"\n",
    "    m = np.shape(X)[0]\n",
    "    a_vet,z,Y,Theta_vec = ForwardProp(nn_params, layers_data, X, Y)  #computing the units values, the activated values z, the label matrix Y and the weights matrices\n",
    "\n",
    "    delta = [0 for i in range(len(layers_data)-1)]                   # initialise the little delta list\n",
    "    for i in reversed(range(1,len(layers_data))):\n",
    "        if i == len(layers_data)-1:\n",
    "            delta[i-1] = a_vet[-1]-Y                                # this is the value of the delta_out\n",
    "        else:\n",
    "            delta[i-1]= (Theta_vec[i][:,1:].T@delta[i])*sigmoidGradient(z[i-1]) #formula for computing the inner delta.\n",
    "\n",
    "    Delta = [0 for i in range(len(layers_data)-1)]        # Initialise the big delta list. \n",
    "    for i in reversed(range(0, len(layers_data)-1)):\n",
    "        Delta[i] = delta[i]@a_vet[i].T                      # The entry i is a matrix with same shape of the weights matrix i\n",
    "\n",
    "    reg_grad = [(Lambda/m)*(np.hstack((np.zeros(np.shape(Theta)[0])[np.newaxis].T, Theta[:,1:]))) for Theta in Theta_vec] #regularisation factors\n",
    "    Theta_grad =[(1/m)*Del+reg for Del,reg in zip(Delta,reg_grad)]                     # gradient matrices\n",
    "    grad_vec = np.concatenate([Theta_g.reshape(Theta_g.size,order = 'F') for Theta_g in Theta_grad]) #gradients vector\n",
    "\n",
    "    return grad_vec\n",
    "\n",
    "\n",
    "def NN_fit(X, Y, hidden_layer_sizes = [2], max_iter = 50, Lambda = 0):\n",
    "    \"\"\"\n",
    "    The function will train a Neural Network Classifier on your set X with labels in the vector Y.\n",
    "    It returns a single vector that containing all the optimal weights for the classifier after max_iter iterations\n",
    "    of the optimisation function fmin_cg. To use it for the prediction you need to reshape all the Weights matrix.\n",
    "    The function reshape_thetas will do it for you.\n",
    "\n",
    "    You have the following options:\n",
    "\n",
    "           -hidden_layers_sizes (optional) : is a vector that contains the numbers of units in each internal\n",
    "                                             layer of the Neural Network. By default the Neural Network has\n",
    "                                             a single internal layer with two units. More entries in the vector\n",
    "                                             means more layers.\n",
    "\n",
    "           -max_iter(optional): it determines the maximum number of iterations that the function fmin_cg must\n",
    "                                perform.\n",
    "\n",
    "           -Lambda: is the regularisation parameter. By default is zero. Increase the parameter if you are \n",
    "                    experiencing high variance or reduce it if you have high bias.\n",
    "    \"\"\"\n",
    "\n",
    "          #### UTILITIES ####\n",
    "    input_layer_size = np.shape(X)[1]        #recovering the input units from the experimental matrix\n",
    "    num_labels = len(np.unique(Y))           #recovering the number of labels from the data stored in the label vector Y\n",
    "    layers_data = [input_layer_size] + hidden_layer_sizes + [num_labels]         # a vector that contains the info about the units of all the layers\n",
    "\n",
    "\n",
    "            #### INITIALISATION OF THE PARAMETERS ####\n",
    "    Thetas = [Rand_param_Theta(layers_data[i], layers_data[i+1]) for i in range(len(layers_data)-1)]         #random initialisation of the layers matrices Weights\n",
    "    nn_params_init = np.concatenate([Theta_p.reshape(Theta_p.size,order = 'F') for Theta_p in Thetas])       #it creates a vector with all the entries of the Weights\n",
    "\n",
    "           #### FITTING OF THE NEURAL NETWORK ####\n",
    "    args = (layers_data, X, Y, Lambda)         #arguments to pass to the functions\n",
    "    xopt = fmin_cg(cost_func_NN, x0 = nn_params_init, fprime = grad_NN, args = args, maxiter = max_iter)  #computes the optimal Weights vector\n",
    "\n",
    "    return xopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Testing ##\n",
    "We now test our implementation on a dataset provided by Andrew NG. The aim is to train the Neural Network in order to recognise hand writing. We have a $5000\\times400$ matrix $X$ as training set and a $1\\times5000$ label vector $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.050756\n",
      "         Iterations: 100\n",
      "         Function evaluations: 252\n",
      "         Gradient evaluations: 252\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "data = loadmat('ex4data1.mat')\n",
    "layers_data = [25]\n",
    "X = data['X']\n",
    "Y = data['y']\n",
    "Theta_opt = NN_fit(X,Y,layers_data, max_iter = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now that we have the weights matrices we look how good the Neural Netwrok perform on the the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.8"
      ]
     },
     "execution_count": 223,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = nn_predict(Theta_opt, [400,25,10],X, 1)\n",
    "Y_train = Y.reshape(Y.size)\n",
    "round((pred == Y_train).mean()*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For a more realistic result we need to split our set and see what happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.027718\n",
      "         Iterations: 100\n",
      "         Function evaluations: 266\n",
      "         Gradient evaluations: 266\n"
     ]
    }
   ],
   "source": [
    "layers_data = [25]\n",
    "Theta_opt = NN_fit(X_train,Y_train,layers_data, max_iter = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.32"
      ]
     },
     "execution_count": 227,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = nn_predict(Theta_opt, [400,25,10],X_valid, 1)\n",
    "Y_valid_aux = Y_valid.reshape(Y_valid.size)\n",
    "round((pred == Y_valid_aux).mean()*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "That is still a nice performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}